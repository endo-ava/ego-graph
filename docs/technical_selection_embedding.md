# 技術選定レポート: Embedding Model

## 1. 概要
EgoGraphプロジェクトにおいて、個人の全データ（日本語主体、一部英語）をベクトル化するためのモデル選定レポートです。
実行環境は **GitHub Actions（完全ローカル実行）** を前提とし、「ストレージ容量」「推論速度」「日本語性能」「ライセンス」の観点から比較・推奨を行います。
(現在日時: 2025年12月時点)

## 2. 候補モデル比較

| モデル名 | リリース時期 | パラメータ数 (サイズ) | 言語対応 | 特徴 | GA実行 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1. cl-nagoya/ruri-v3-310m** | 2025年4月 | 310M (約0.6~1.2GB) | **日本語特化** | 国産SOTA。文脈8192トークン対応。 | ◎ |
| **2. EmbeddingGemma** | 2025年9月 | 308M (約0.6GB) | 多言語 (100+) | Google製。オンデバイス特化。Matryoshka対応。 | ◎ |
| **3. nomic-embed-text-v2-moe** | 2025年4月 | 不定 (MoE) | 多言語 | MoE(混合エキスパート)構成。高性能だが依存関係が複雑な可能性。 | △ |
| **4. multilingual-e5-small** | 2023年中盤 | 118M (約0.5GB) | 多言語 | 枯れた技術。安定的だが性能は2025年基準では劣る。 | ○ |

---

## 3. 詳細評価

### 1. cl-nagoya/ruri-v3-310m (推奨No.1)
*   **開発元**: 名古屋大学 (cl-nagoya)
*   **リリース**: 2025年4月16日
*   **概要**: 日本語汎用Embeddingモデルの決定版。ModernBERT-Jaをベースにしており、日本語のJMTEBベンチマークで最高水準（Avg 77.2）を記録。
*   **強み**:
    *   **日本語理解力**: 圧倒的です。口語や日本独自の言い回しにも強い。
    *   **ロングコンテキスト**: 8192トークンまで扱えるため、長いメールやドキュメントも切り詰めずにベクトル化可能。
    *   **サイズ**: 310MパラメータはGitHub ActionsのCPUでも十分に実用的な速度で動作します。
*   **弱点**: 英語オンリーのドキュメントに対する性能はMultilingualモデルに劣る可能性があります（基本は日本語モデル）。

### 2. EmbeddingGemma (推奨No.2)
*   **開発元**: Google DeepMind
*   **リリース**: 2025年9月4日
*   **概要**: Gemma 3アーキテクチャベースの埋め込みモデル。オンデバイス（PCやスマホ）での動作を最優先に設計されています。
*   **強み**:
    *   **Matryoshka Representation Learning (MRL)**: ベクトルの次元数を柔軟に（例えば768→256へ）小さくしても精度が落ちにくい技術に対応。DB容量を節約したい場合に有利。
    *   **モダンな設計**: 2025年後半の最新アーキテクチャ。
*   **弱点**: 日本語特化のRuriに比べると、日本語特有のニュアンス理解では一歩譲る可能性があります。

### 3. nomic-embed-text-v2-moe
*   **開発元**: Nomic AI
*   **リリース**: 2025年4月
*   **概要**: MoE（Mixture of Experts）を採用した高品質モデル。
*   **評価**: 性能は高いですが、MoEアーキテクチャを実行するためのライブラリ依存やメモリ管理がGitHub Actions上ではトラブルの元になりやすいため、今回は次点とします。

### 4. multilingual-e5-small
*   **開発元**: Intfloat (Microsoft系)
*   **リリース**: 2023年
*   **概要**: 長い間デファクトスタンダードだったモデル。
*   **評価**: 安定性は抜群ですが、2025年の最新モデル（Ruri-v3やGemma）と比較すると、同じメモリ効率で得られる精度に見劣りします。あえて今選ぶ理由は薄いです。

---

## 4. 結論・推奨

**結論: `cl-nagoya/ruri-v3-310m` を採用することを強く推奨します。**

*   **理由1（日本語性能）**: EgoGraphは個人のライフログ（日本語）が中心であるため、日本語特化SOTAであるRuriの恩恵が最大化されます。
*   **理由2（コンテキスト長）**: 8192トークン対応は、将来的に長文のメールやWeb記事をRAGに取り込む際に大きなアドバンテージになります。
*   **理由3（サイズ感）**: 310MパラメータはGitHub Actions（CPU）でのバッチ処理と、精度（賢さ）のバランスがベストです。

もし「英語の技術ドキュメント」の比重が極めて高くなるようであれば、多言語対応の `EmbeddingGemma` ローカル実行への切り替えを検討してください。
