name: GitHub Worklog Ingestion to R2 Data Lake

on:
  schedule:
    # 1æ—¥1å›žå®Ÿè¡Œï¼ˆæ·±å¤œï¼‰
    - cron: '0 15 * * *'  # 15:00 UTC = 00:00 JST (æ·±å¤œ)
  workflow_dispatch:

jobs:
  ingest:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python 3.13
        run: uv python install 3.13

      - name: Install dependencies
        run: |
          uv sync --all-packages

      - name: Run GitHub Worklog ingestion pipeline
        env:
          GITHUB_PAT: ${{ secrets.EGOGRAPH_GITHUB_PAT }}
          GITHUB_LOGIN: ${{ secrets.EGOGRAPH_GITHUB_LOGIN }}
          DUCKDB_PATH: data/analytics.duckdb
          R2_ENDPOINT_URL: ${{ secrets.R2_ENDPOINT_URL }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
          LOG_LEVEL: INFO
        run: |
          [ -n "$GITHUB_PAT" ] || { echo "EGOGRAPH_GITHUB_PAT secret is required"; exit 1; }
          [ -n "$GITHUB_LOGIN" ] || { echo "EGOGRAPH_GITHUB_LOGIN secret is required"; exit 1; }

          uv run python -m ingest.github.main

      - name: Verify Parquet Data Integrity (R2)
        if: success()
        env:
          R2_ENDPOINT_URL: ${{ secrets.R2_ENDPOINT_URL }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          uv run python - << 'PY'
          import duckdb
          import os

          # R2è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ç›´æŽ¥èª­ã¿å–ã‚Š
          endpoint_url = os.environ["R2_ENDPOINT_URL"]
          access_key_id = os.environ["R2_ACCESS_KEY_ID"]
          secret_access_key = os.environ["R2_SECRET_ACCESS_KEY"]
          bucket_name = os.environ["R2_BUCKET_NAME"]
          events_path = os.environ.get("R2_EVENTS_PATH", "events/")

          access_key_id = access_key_id.replace("'", "''")
          secret_access_key = secret_access_key.replace("'", "''")
          endpoint_clean = endpoint_url.replace("https://", "").replace("'", "''")

          conn = duckdb.connect(":memory:")
          conn.execute("INSTALL httpfs;")
          conn.execute("LOAD httpfs;")
          conn.execute(f"""
              CREATE SECRET (
                  TYPE S3,
                  KEY_ID '{access_key_id}',
                  SECRET '{secret_access_key}',
                  REGION 'auto',
                  ENDPOINT '{endpoint_clean}',
                  URL_STYLE 'path'
              );
          """)

          bucket_clean = bucket_name.replace("'", "''")
          events_clean = events_path.replace("'", "''")
          parquet_url = f"s3://{bucket_clean}/{events_clean}github/commits/**/*.parquet"
          parquet_url_escaped = parquet_url.replace("'", "''")
          print(f"ðŸ§ Querying: {parquet_url}")

          try:
              count = conn.execute(f"SELECT COUNT(*) FROM read_parquet('{parquet_url_escaped}')").fetchone()[0]
              latest = conn.execute(f"SELECT MAX(committed_at_utc) FROM read_parquet('{parquet_url_escaped}')").fetchone()[0]

              print(f"âœ… Total commits in R2: {count}")
              print(f"âœ… Latest commit in R2: {latest}")
          except Exception as e:
              print(f"âš ï¸ Verification failed or no data: {e}")
              # åˆå›žå®Ÿè¡Œæ™‚ã¯ãƒ‡ãƒ¼ã‚¿ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ã«ã¯ã›ãšè­¦å‘Šã«ç•™ã‚ã‚‹
          finally:
              conn.close()
          PY

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: |
            *.log
            logs/
          retention-days: 7
          if-no-files-found: ignore
