name: Spotify Data Ingestion to R2 Data Lake

on:
  schedule:
    # 1Êó•2ÂõûÂÆüË°åÔºà50Êõ≤‰ª•‰∏ä„ÅÆÂèñ„Çä„Åì„Åº„Åó„ÇíÈò≤„ÅêÔºâ
    - cron: '0 2 * * *'   # 02:00 UTC = 11:00 JST (ÂçàÂâç)
    - cron: '0 14 * * *'  # 14:00 UTC = 23:00 JST (Â§ú)
  workflow_dispatch: # ÊâãÂãï„Éà„É™„Ç¨„Éº„ÇíË®±ÂèØ

jobs:
  ingest:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python 3.13
        run: uv python install 3.13

      - name: Install dependencies
        run: |
          uv sync --all-packages

      - name: Run Spotify DuckDB ingestion pipeline
        env:
          SPOTIFY_CLIENT_ID: ${{ secrets.SPOTIFY_CLIENT_ID }}
          SPOTIFY_CLIENT_SECRET: ${{ secrets.SPOTIFY_CLIENT_SECRET }}
          SPOTIFY_REFRESH_TOKEN: ${{ secrets.SPOTIFY_REFRESH_TOKEN }}
          DUCKDB_PATH: data/analytics.duckdb
          R2_ENDPOINT_URL: ${{ secrets.R2_ENDPOINT_URL }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
          LOG_LEVEL: INFO
        run: |
          uv run python -m ingest.spotify_r2_main

      - name: Verify Parquet Data Integrity (R2)
        if: success()
        env:
          R2_ENDPOINT_URL: ${{ secrets.R2_ENDPOINT_URL }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          uv run python - << 'PY'
          import duckdb
          import os

          # R2Ë®≠ÂÆö„ÇíÁí∞Â¢ÉÂ§âÊï∞„Åã„ÇâÁõ¥Êé•Ë™≠„ÅøÂèñ„Çä
          endpoint_url = os.environ["R2_ENDPOINT_URL"]
          access_key_id = os.environ["R2_ACCESS_KEY_ID"]
          secret_access_key = os.environ["R2_SECRET_ACCESS_KEY"]
          bucket_name = os.environ["R2_BUCKET_NAME"]
          events_path = os.environ.get("R2_EVENTS_PATH", "events/")

          conn = duckdb.connect(":memory:")
          conn.execute("INSTALL httpfs; LOAD httpfs;")
          conn.execute(f"""
              CREATE SECRET (
                  TYPE S3,
                  KEY_ID '{access_key_id}',
                  SECRET '{secret_access_key}',
                  REGION 'auto',
                  ENDPOINT '{endpoint_url.replace("https://", "")}',
                  URL_STYLE 'path'
              );
          """)

          parquet_url = f"s3://{bucket_name}/{events_path}spotify/plays/**/*.parquet"
          print(f"üßê Querying: {parquet_url}")

          try:
              count = conn.execute(f"SELECT COUNT(*) FROM read_parquet('{parquet_url}')").fetchone()[0]
              latest = conn.execute(f"SELECT MAX(played_at_utc) FROM read_parquet('{parquet_url}')").fetchone()[0]

              print(f"‚úÖ Total plays in R2: {count}")
              print(f"‚úÖ Latest play in R2: {latest}")
          except Exception as e:
              print(f"‚ö†Ô∏è Verification failed or no data: {e}")
              # ÂàùÂõûÂÆüË°åÊôÇ„ÅØ„Éá„Éº„Çø„Åå„Å™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅ„Ç®„É©„Éº„Å´„ÅØ„Åõ„ÅöË≠¶Âëä„Å´Áïô„ÇÅ„Çã
          finally:
              conn.close()
          PY

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: |
            *.log
            logs/
          retention-days: 7
          if-no-files-found: ignore
